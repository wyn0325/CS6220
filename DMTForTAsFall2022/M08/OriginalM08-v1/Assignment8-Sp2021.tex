%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. Document class 
\documentclass[letter-paper,12pt]{article} % This defines the style of your paper
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. Packages
\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor={blue},
}

\usepackage{xcolor}
\usepackage{multirow} % Multirow is for tables with multiple rows within one cell.
\usepackage{booktabs} % For even nicer tables.
\usepackage{graphicx} 
\usepackage{setspace}
\setlength{\parindent}{0in}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{url}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{subfigure}
\usepackage{subcaption}

\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\bfseries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. Header (and Footer)
\pagestyle{fancy} % With this command we can customize the header style.
\fancyhf{} % This makes sure we do not have other information in our header or footer.

% \title{CS 6220 Data Mining --- Assignment 8A}
% \date{}

\begin{document}
% \thispagestyle{empty} % This command disables the header on the first page. 
% \maketitle

% \hline
\begin{center}
\begin{Huge}
CS 6220 Data Mining --- Assignment 8
\end{Huge}
\end{center}

\hline
\hline
~\\~\\~\\

\begin{center}
\begin{Large}
\textbf{Decision Tree}
\end{Large}
\end{center}
~\\~\\

This assignment will require you to implement and interpret the concepts of decision tree. Keep in mind that the main objective of this assignment is to highlight the insights that we can derive from applying these techniques—the coding aspect is secondary. Accordingly, you are welcome to consult any online documentation and/or code that has been posted to the course website, so long as all references and sources are properly cited. You are also encouraged to use code libraries, so long as you acknowledge any source code that was not written by you by mentioning the original author(s) directly in your source code (comment or header).\\

\textbf{Objectives:}
\begin{enumerate}
    \item Implement a decision tree using scikit learn.
    \item Display the final decision tree.
    \item Interpret results generated by the training process. 
\end{enumerate}
~\\

\textbf{Submission:}\\

Submit your ipynb file on the Assignment submission portal. \\~\\


\textbf{Grading Criteria:}\\
Follow the instructions in the pdf, and complete each task. You will be graded on the application of the modules’ topics, the completeness of your answers to the questions in the assignment notebook, and the clarity of your writing and code.\\~\\


\newpage

\begin{center}
    \Large \textbf{Assignment Description}
\end{center}

\textbf{The Data}\\
The given dataset contains information of Boston housing-prices and you are already familiar with it. The dataset has 13 numeric/categorical predictive attributes. Median Value (attribute 14) is usually the target.\\
The dataset can be loaded using the sklearn datasets module. \\


\textbf{What to Do}\\
First, load the dataset from sklearn dataset using the following code snippet. You can get the features name using \textit{data.feature\_names} and target names using \textit{data.target\_names}\\

\texttt{from sklearn.datasets import load\_boston\\
data = load\_boston()\\
X = data.data\\
y = data.target\\
}

Note here that this dataset as it is, is good for regression. To convert it into a classification dataset, 
\begin{itemize}
\item Split the range of target values into three equal parts - low, mid, and high.
\item Reassign the target values into into three categorical values 0, 1, and 2, representing low, mid and high range of values, respectively.  
\end{itemize}

Then, 

\begin{enumerate}
    \item Split the dataset into 70\% training set and 30\% test set.

    \item Using scikit-learn’s DecisionTreeClassifier, train a supervised learning model that can be used to generate predictions for your data. A reference to how you can do that can be found in the users manual at \url{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier}.
    
    \item Report the tree depth, number of leaves, feature importance, train score, and test score of the tree.  Let the tree depth be Td. 
    
    \item Show the visual output of the decision tree. 
    
    \item   Next, Generate (Td-1) decision trees on the same training set using fixed tree depths $\{1, 2, ... (Td-1)\}$. The tree depth can be set using \textit{max\depth=d}, where $d$ is the depth of the tree.
    
    \item For each of the (Td-1) trees report, tree depth, number of leaves, feature importance, train score, and test score of the tree. 
    
    \item Show the visual output of the decision tree with highest test score from the (Td-1) trees.
    
    To visualize the decision tree, use \href{https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart}{Graphviz} library. You can find details in this \href{https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart}{link}. Show the feature names and class names in the visualization.\\
\end{enumerate}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
    \item Report of tree depth, number of leaves, feature importance, train score, and test score of the decision tree. 
    
    \item Report of tree depth, feature importance, train score, and test score of the decision tree for each  value of tree depth used in step 5. 
    
    \item Visual output of the decision trees. There should be two separate decision trees as mentioned previously. 
\end{itemize}


\end{document}